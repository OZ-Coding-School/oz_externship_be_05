from datetime import datetime, timedelta, timezone
from typing import Any, Iterable, Iterator, List, TypeVar, Tuple, Type

from django.core.management.base import BaseCommand
from django.db.models import Model
from mypy_boto3_s3 import S3Client
from mypy_boto3_s3.type_defs import DeleteTypeDef, ObjectIdentifierTypeDef
# ê´€ë¦¬í•  ëª¨ë¸ ì„í¬íŠ¸
from apps.qna.models.answer.images import AnswerImage


from apps.core.utils.s3_client import S3Client as MyS3ClientWrapper

T = TypeVar("T")


def chunked(iterable: Iterable[T], size: int = 1000) -> Iterator[List[T]]:
    source = list(iterable)
    for i in range(0, len(source), size):
        yield source[i : i + size]


class Command(BaseCommand):
    help = "Remove orphaned answer images from S3"

    def add_arguments(self, parser: Any) -> None:
        parser.add_argument(
            "--dry-run",
            action="store_true",
            help="ì‹¤ì œ ì‚­ì œëŠ” ìˆ˜í–‰í•˜ì§€ ì•Šê³ , ì‚­ì œ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.",
        )

    def handle(self, *args: Any, **options: Any) -> None:
        is_dry_run = options["dry_run"]
        mode_text = "[DRY RUN]" if is_dry_run else "[REAL]"
        self.stdout.write(f"{mode_text} ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        wrapper = MyS3ClientWrapper()
        s3_client: S3Client = wrapper.s3
        bucket_name = wrapper.bucket_name

        safety_boundary = datetime.now(timezone.utc) - timedelta(hours=24)
        # ì²­ì†ŒëŒ€ìƒ ëª©ë¡
        targets: List[Tuple[str, Type[Model]]] = [
            ("answer_images/", AnswerImage),

        ]
        total_scanned = 0
        total_deleted = 0

        for prefix, model_class in targets:
            self.stdout.write(f"\nğŸš€ [{prefix}] êµ¬ì—­ ìŠ¤ìº” ì¤‘... ({model_class.__name__})")


            paginator = s3_client.get_paginator("list_objects_v2")
            pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

            for page in pages:
                if "Contents" not in page:
                    continue

                orphans, scanned_count = self._find_orphans_in_page(
                    page["Contents"], 
                    safety_boundary,
                    model_class
                )
                
                total_scanned += scanned_count

                if not orphans:
                    continue

                deleted_count = self._process_batch_deletion(s3_client, bucket_name, orphans, is_dry_run)
                total_deleted += deleted_count

        self.stdout.write(self.style.SUCCESS(f"ì‘ì—… ì¢…ë£Œ! ì´ ìŠ¤ìº”: {total_scanned}, ì´ ì‚­ì œ: {total_deleted}"))

    def _find_orphans_in_page(
            self, 
            contents: List[Any], 
            safety_boundary: datetime,
            model_class: Type[Model]
    ) -> tuple[List[str], int]:
        
        candidates: List[str] = []
        scanned = 0

        for obj in contents:
            scanned += 1
            if obj.get("LastModified") >= safety_boundary:
                continue

            key = obj.get("Key")
            if key:
                candidates.append(key)

        if not candidates:
            return [], scanned

        # ëª¨ë“  ëª¨ë¸ì˜ í•„ë“œê°€ image_url
        existing_keys = set(model_class.objects.filter(image_url__in=candidates).values_list("image_url", flat=True))
        orphan_keys = set(candidates) - existing_keys
        return list(orphan_keys), scanned

    def _process_batch_deletion(self, s3: S3Client, bucket: str, keys: List[str], is_dry_run: bool) -> int:
        deleted_count = 0

        for chunk in chunked(keys, size=1000):
            if is_dry_run:
                self.stdout.write(self.style.WARNING(f"ğŸ‘€ [Dry Run] ì‚­ì œ ëŒ€ìƒ ë°œê²¬: {len(chunk)}ê°œ (ì‹¤í–‰ ì•ˆ ë¨)"))
                deleted_count += len(chunk)
                continue

            count = self._delete_chunk_safely(s3, bucket, chunk)
            deleted_count += count

        return deleted_count

    def _delete_chunk_safely(self, s3: S3Client, bucket: str, chunk: List[str]) -> int:
        try:
            objects: List[ObjectIdentifierTypeDef] = [{"Key": k} for k in chunk]
            delete_req: DeleteTypeDef = {"Objects": objects}

            response = s3.delete_objects(Bucket=bucket, Delete=delete_req)

            deleted = len(response.get("Deleted", []))
            errors = response.get("Errors", [])

            if errors:
                for err in errors:
                    self.stdout.write(self.style.ERROR(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {err.get('Key')} ({err.get('Code')})"))

            self.stdout.write(f"ğŸ”¥ {deleted}ê°œ ì‚­ì œ ì™„ë£Œ")
            return deleted

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"ğŸ’¥ S3 API í˜¸ì¶œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}"))
            return 0
